{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbennett14dacode/all-the-test/blob/main/Copy_of_Week_4_Workalong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 4 - AI\n",
        "\n",
        "\n",
        "We are going to spend the time this week working on how we can use AI in Colab.\n",
        "\n",
        "Before we click the _Run_ button on our cell we are going to need to **switch our runtime** so that we are using one that has some AI hardware support.\n",
        "\n",
        "Under your picture there is a drop down arrow next to _Connect_. Click on that and select _Change runtime type_ and select _T4 GPU_.\n",
        "\n",
        "Basically AI tools do the same kind of math that graphics cards do, so you if you use a graphics card you can get results much faster."
      ],
      "metadata": {
        "id": "WkKSQKw-TmJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers Library\n",
        "\n",
        "Transformers is a Python Library provided by [hugging face](https://huggingface.co/) which is a platform that hosts a collective of generative AI tools. We could probably spend all 4 weeks of this class exploring it but we are just going to try a few things."
      ],
      "metadata": {
        "id": "GR9MYD_PUiyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarzing PDFs\n",
        "\n",
        "In week 3 we did some text analysis on a collection of PDFs. We are going to use those same files. The next cell will install all the libraries that we need to do this using AI.\n",
        "\n",
        "**Please note** The next cell will take a least a couple of minutes to run! It took 2 minutes for me!"
      ],
      "metadata": {
        "id": "66Putbo7Vy6x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxZpRsPdTSm-"
      },
      "outputs": [],
      "source": [
        "# install these libraries since they aren't available be default\n",
        "!pip install pypdf\n",
        "!pip install transformers\n",
        "\n",
        "#Colab has a special library just for working with files on google drive\n",
        "from google.colab import drive\n",
        "\n",
        "from pypdf import PdfReader\n",
        "import pandas as pd\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "#Let's ignore the pypdf errors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#this is the new transformers stuff\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "#Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "print(\"Libraries and Data loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The Model\n",
        "\n",
        "We'll be using [bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) to do the summaries with the transformers library. I won't explain the code line by line. This is a bit 'hand-wavy' I realize but there is plenty of material online that will help you understand what *exactly* is going on here if you are curious.\n",
        "\n",
        "Basically what we are doing is downloading components of a large language model (gigabytes worth) that has been pretrained and we are creating a function that when given text, returns a summary of that text."
      ],
      "metadata": {
        "id": "GYC_KNhLis8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup our pieces (this will also take some time)\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "#We'll create a function that does the summary\n",
        "def generate_summary(text):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "IvN4g7oNf8KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Summarize each file\n",
        "\n",
        "We'll now loop through each PDF in our `LibraryJuicePython` directory, extract the text, and then create a dataframe of those summaries.\n",
        "\n",
        "\n",
        "This cell will probably take a long time to run, especially if you have a lot of PDF files. _Like minutes worth of waiting!_ Seriously you should probably grab a coffee."
      ],
      "metadata": {
        "id": "L4W30jggX7LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = []\n",
        "\n",
        "for file in glob.glob(\"/content/gdrive/MyDrive/LibraryJuicePython/*.pdf\"):\n",
        "\n",
        "  filename = file.split(\"/\")[-1]\n",
        "  print(\"Generating Summary for... \",filename)\n",
        "  text = \"\"\n",
        "  reader = PdfReader(file)\n",
        "  for page in reader.pages:\n",
        "    text += page.extract_text()\n",
        "\n",
        "  #call our newly made function\n",
        "  summary = generate_summary(text)\n",
        "  summaries.append([filename,summary])\n",
        "\n",
        "#Turn our summaries into a dataframe.\n",
        "summary_data = pd.DataFrame(summaries)\n",
        "summary_data.columns = [\"Filename\",\"Summary\"]\n",
        "print(\"Finished!\")"
      ],
      "metadata": {
        "id": "oYBoHqOAYESJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look at our final dataframe\n",
        "summary_data"
      ],
      "metadata": {
        "id": "ip0zAIF6iKNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's now save this summary information in our directory as a CSV file\n",
        "summary_data.to_csv(\"/content/gdrive/MyDrive/LibraryJuicePython/adv_week4_pdf_summaries.csv\", index=False)"
      ],
      "metadata": {
        "id": "Kt0FCEZEkSvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# You can stop here!\n",
        "\n",
        "I've asked you to be pretty patient while you watch code running. Feel free to finish the notebook here. The next section is not necessary but more for fun.\n"
      ],
      "metadata": {
        "id": "FFg_4dgVrlYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## AI for image generation\n",
        "\n",
        "The first activity for this week was, let's say interesting, but dense in code. Let's just finish off by having some fun with image generation.\n",
        "\n",
        "This is another cell that will take a bit to run. Go grab another coffee I'd say. All the bars will need to turn green before it is done. Usually this cell will take 5 minutes to load."
      ],
      "metadata": {
        "id": "duewUNFVlDIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from IPython.display import display, Markdown\n",
        "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-2\"\n",
        "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "cbc3Hbs_l0VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell to grab a random summary from your dataframe\n",
        "#Keep going until you have one you like before proceeding to the next cell\n",
        "random_item = summary_data.sample(1)\n",
        "file_name = random_item[\"Filename\"].values[0]\n",
        "random_summary = random_item[\"Summary\"].values[0]\n",
        "print(file_name)\n",
        "print(random_summary)"
      ],
      "metadata": {
        "id": "QXpaRO5enxKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We'll give your summary to the imagine generation\n",
        "#If you run this cell again you'll get a different picture\n",
        "image = pipe(random_summary).images[0]\n",
        "display(image)"
      ],
      "metadata": {
        "id": "nY7oLtuingqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell to save your image in your usual Google Drive folder\n",
        "image.save(file_name+\".png\")\n",
        "shutil.move(file_name+\".png\",\"/content/gdrive/MyDrive/LibraryJuicePython\")"
      ],
      "metadata": {
        "id": "rAZPlfKQomwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imagine Generation, just for fun\n",
        "\n",
        "I've created a [notebook](https://colab.research.google.com/drive/1Yp78vX9KQtpzo32b1vSWvoPKyksT7b_G?usp=sharing) that uses the Hugging Face Libraries to do image generation. Feel free to check it out, it might give you some ideas on how you can use generative AI with Python."
      ],
      "metadata": {
        "id": "UoZYP2Gsmi6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# On to the homework\n",
        "\n",
        "We can use AI in our Colab Notebooks but we've discovered that we need to be a bit patient with letting the code run... It um, takes time.\n",
        "\n",
        "Head over to the homework to try some more AI stuff."
      ],
      "metadata": {
        "id": "iJCnLcYZqxoR"
      }
    }
  ]
}